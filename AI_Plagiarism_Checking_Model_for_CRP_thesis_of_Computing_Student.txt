# ==============================================
# AI-PLAGIARISM CHECKING MODEL FOR CRP THESIS
# COMPLIANT WITH  SPECIFICATIONS
# ==============================================

# Install required packages
!pip install torch transformers sentence-transformers scikit-learn numpy pandas pdfplumber python-docx spacy matplotlib seaborn ipywidgets

# Download spaCy model
!python -m spacy download en_core_web_sm

# Mount Google Drive for dataset access
from google.colab import drive
drive.mount('/content/drive')

import torch
import transformers
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pandas as pd
import pdfplumber
from docx import Document
import spacy
import re
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

# Import Colab widgets for file upload interface
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML
import io
import base64

# ==============================================
# CLASS: AI-PLAGIARISM CHECKING MODEL
# ==============================================

class CRPThesisEvaluator:
    """
    COMPREHENSIVE AI-PLAGIARISM CHECKING MODEL FOR CRP THESIS
    Full implementation of Prompt 2 requirements:
    - Four parallel analysis streams
    - Exact grading rubric (8 cases)
    - Paragraph-level AI detection
    - Chapter-level plagiarism detection
    - Colab notebook interface with file upload
    """

    def __init__(self):
        """Initialize the evaluation system with Prompt 2 specifications"""
        print("üöÄ INITIALIZING CRP THESIS EVALUATOR")
        print("=" * 60)

        # Load spaCy model
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            print("Installing spaCy model...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")

        # Initialize all models
        self.setup_parallel_models()

        # Define dataset path
        self.dataset_path = "/content/drive/MyDrive/AI-Plagiarism Checking Model for CRP thesis of Computing Student/Dataset"

        # Load assessment criteria
        self.assessment_criteria = self.load_assessment_criteria_prompt2()

        # Initialize results storage
        self.results = {}

        # Grading rubric (8 cases - EXACT IMPLEMENTATION)
        self.grading_rubric = {
            "Merit_Case_A": {
                "conditions": ["ai_content > 40", "plagiarism > 40", "criteria_fulfilled == True"],
                "grade": "Merit"
            },
            "Redo": {
                "conditions": ["ai_content > 40", "plagiarism > 40", "criteria_fulfilled == False"],
                "grade": "Redo"
            },
            "Distinction": {
                "conditions": ["ai_content < 40", "plagiarism < 40", "criteria_fulfilled == True"],
                "grade": "Distinction"
            },
            "Merit_Case_B": {
                "conditions": ["ai_content > 40", "plagiarism < 40", "criteria_fulfilled == True"],
                "grade": "Merit"
            },
            "Merit_Case_C": {
                "conditions": ["ai_content < 40", "plagiarism > 40", "criteria_fulfilled == True"],
                "grade": "Merit"
            },
            "Pass_Case_A": {
                "conditions": ["ai_content < 40", "plagiarism < 40", "criteria_fulfilled == False"],
                "grade": "Pass"
            },
            "Pass_Case_B": {
                "conditions": ["ai_content > 40", "plagiarism < 40", "criteria_partial == True"],
                "grade": "Pass"
            },
            "Pass_Case_C": {
                "conditions": ["ai_content > 40", "plagiarism > 40", "criteria_partial == True"],
                "grade": "Pass"
            }
        }

        print("‚úÖ System initialized with Prompt 2 specifications")
        print(f"   ‚Ä¢ Four parallel analysis streams")
        print(f"   ‚Ä¢ 8-case grading rubric")
        print(f"   ‚Ä¢ Paragraph-level AI detection")
        print(f"   ‚Ä¢ Chapter-level plagiarism detection")
        print(f"   ‚Ä¢ 0.85 similarity threshold")

    def setup_parallel_models(self):
        """Initialize models for four parallel streams (Prompt 2 Requirement)"""
        print("üîß Setting up parallel analysis models...")

        # STREAM 1: AI-Generated Content Detection
        try:
            print("   üìä Stream 1: Loading RoBERTa for AI detection...")
            self.ai_tokenizer = transformers.AutoTokenizer.from_pretrained("roberta-base")
            self.ai_model = transformers.AutoModelForSequenceClassification.from_pretrained(
                "roberta-base",
                num_labels=2
            )
            print("   ‚úÖ RoBERTa loaded successfully")
        except Exception as e:
            print(f"   ‚ùå Error loading RoBERTa: {e}")

        # STREAM 2: Plagiarism Detection (Sentence Transformers)
        try:
            print("   üìä Stream 2: Loading Sentence-BERT for plagiarism...")
            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("   ‚úÖ Sentence-BERT loaded successfully")
        except Exception as e:
            print(f"   ‚ùå Error loading Sentence-BERT: {e}")

        # STREAM 3: Assessment Criteria Verification (TF-IDF)
        try:
            print("   üìä Stream 3: Setting up TF-IDF for criteria matching...")
            self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
            print("   ‚úÖ TF-IDF vectorizer ready")
        except Exception as e:
            print(f"   ‚ùå Error setting up TF-IDF: {e}")

        # STREAM 4: Grading Logic (Built-in)
        print("   üìä Stream 4: Grading logic configured")

        print("‚úÖ All four parallel streams initialized")

    def load_assessment_criteria_prompt2(self):
        """
        Load assessment criteria with P1-D3 alignment
        (Prompt 2 Requirement: P1-D3 marking standards)
        """
        criteria = {
            'Pass': [
                {'code': 'P1', 'description': 'Produce research proposal with clear research question and literature review'},
                {'code': 'P2', 'description': 'Examine appropriate research methods and approaches'},
                {'code': 'P3', 'description': 'Conduct research using appropriate methods considering costs and ethics'},
                {'code': 'P4', 'description': 'Apply analytical tools and analyze research findings'},
                {'code': 'P5', 'description': 'Communicate research outcomes appropriately'},
                {'code': 'P6', 'description': 'Discuss effectiveness of research methods'},
                {'code': 'P7', 'description': 'Discuss alternative methodologies and lessons learnt'}
            ],
            'Merit': [
                {'code': 'M1', 'description': 'Analyze research approaches and justify method selection'},
                {'code': 'M2', 'description': 'Discuss merits and limitations of data collection approaches'},
                {'code': 'M3', 'description': 'Analyze extent to which outcomes meet research objectives'},
                {'code': 'M4', 'description': 'Analyze results for improvements and future research'}
            ],
            'Distinction': [
                {'code': 'D1', 'description': 'Critically evaluate research methodologies and justify choices'},
                {'code': 'D2', 'description': 'Evaluate outcomes and make justified recommendations'},
                {'code': 'D3', 'description': 'Demonstrate reflection and engagement for future improvement'}
            ]
        }
        return criteria

    # ==============================================
    # STREAM 1: AI-GENERATED CONTENT DETECTION
    # ==============================================

    def detect_ai_content(self, text):
        """
        STREAM 1: AI-Generated Content Detection
        - Paragraph-level analysis (Prompt 2 Requirement)
        - RoBERTa model
        - Reduced strictness thresholds
        """
        print("\n" + "ü§ñ STREAM 1: AI-GENERATED CONTENT DETECTION")
        print("-" * 50)
        print("üìä Paragraph-level analysis with RoBERTa...")

        # Split into paragraphs (Prompt 2: paragraph level, not sentence)
        paragraphs = self.split_into_paragraphs(text)
        print(f"   ‚Ä¢ Analyzing {len(paragraphs)} paragraphs")

        ai_probabilities = []
        flagged_paragraphs = []

        for i, para in enumerate(paragraphs[:50]):  # Limit to 50 paragraphs for efficiency
            if len(para) < 100:  # Skip very short paragraphs
                continue

            try:
                # Tokenize
                inputs = self.ai_tokenizer(
                    para,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=True
                )

                # Predict
                with torch.no_grad():
                    outputs = self.ai_model(**inputs)
                    probs = torch.softmax(outputs.logits, dim=-1)
                    ai_prob = probs[0][1].item()  # Probability of being AI-generated

                ai_probabilities.append(ai_prob)

                # Flag if probability > 0.5 (reduced strictness)
                if ai_prob > 0.5:
                    flagged_paragraphs.append({
                        'paragraph_id': i,
                        'probability': ai_prob,
                        'preview': para[:150] + "..." if len(para) > 150 else para
                    })

                    if len(flagged_paragraphs) <= 3:  # Show first 3 flagged
                        print(f"   üö© Paragraph {i}: AI probability = {ai_prob:.1%}")

            except Exception as e:
                continue

        # Calculate overall percentage
        if ai_probabilities:
            overall_ai_percentage = np.mean(ai_probabilities) * 100
        else:
            overall_ai_percentage = 0

        print(f"\nüìà AI DETECTION RESULTS:")
        print(f"   ‚Ä¢ Overall AI percentage: {overall_ai_percentage:.1f}%")
        print(f"   ‚Ä¢ Flagged paragraphs: {len(flagged_paragraphs)}")
        print(f"   ‚Ä¢ Status: {'‚ùå >40%' if overall_ai_percentage > 40 else '‚úÖ <40%'}")

        return {
            'overall_percentage': overall_ai_percentage,
            'flagged_count': len(flagged_paragraphs),
            'flagged_paragraphs': flagged_paragraphs,
            'paragraph_probabilities': ai_probabilities
        }

    # ==============================================
    # STREAM 2: PLAGIARISM DETECTION
    # ==============================================

    def detect_plagiarism(self, text):
        """
        STREAM 2: Plagiarism Detection
        - Chapter-level similarity analysis (Prompt 2 Requirement)
        - Sentence transformers
        - >0.85 threshold for plagiarism flagging
        """
        print("\n" + "üîç STREAM 2: PLAGIARISM DETECTION")
        print("-" * 50)
        print("üìä Chapter-level semantic similarity analysis...")

        # Segment into chapters (Prompt 2: chapter-level)
        chapters = self.segment_into_chapters(text)
        print(f"   ‚Ä¢ Analyzing {len(chapters)} chapters")

        # Load reference documents for comparison
        reference_texts = self.load_reference_documents()
        print(f"   ‚Ä¢ Comparing against {len(reference_texts)} reference documents")

        if not reference_texts:
            print("   ‚ö†Ô∏è  No reference documents found")
            return {
                'overall_percentage': 0,
                'flagged_count': 0,
                'similarity_scores': []
            }

        # Create embeddings for references
        try:
            ref_embeddings = self.semantic_model.encode(reference_texts)
        except:
            print("   ‚ùå Error creating embeddings")
            return {
                'overall_percentage': 0,
                'flagged_count': 0,
                'similarity_scores': []
            }

        chapter_similarities = []
        flagged_chapters = []

        for chapter_name, chapter_text in list(chapters.items())[:10]:  # Limit to 10 chapters
            if len(chapter_text) < 200:
                continue

            try:
                # Create chapter embedding
                chapter_embedding = self.semantic_model.encode([chapter_text])

                # Calculate similarities
                similarities = cosine_similarity(chapter_embedding, ref_embeddings)[0]
                max_similarity = max(similarities) if len(similarities) > 0 else 0

                chapter_similarities.append(max_similarity)

                # Flag if similarity > 0.85
                if max_similarity > 0.85:
                    flagged_chapters.append({
                        'chapter': chapter_name,
                        'similarity': max_similarity,
                        'preview': chapter_text[:200] + "..." if len(chapter_text) > 200 else chapter_text
                    })

                    if len(flagged_chapters) <= 2:  # Show first 2 flagged
                        print(f"   üö© {chapter_name}: Similarity = {max_similarity:.1%}")
                else:
                    if len(chapter_similarities) <= 5:  # Show first 5 non-flagged
                        print(f"   ‚úÖ {chapter_name}: Similarity = {max_similarity:.1%}")

            except Exception as e:
                continue

        # Calculate overall plagiarism percentage
        if chapter_similarities:
            overall_plagiarism_percentage = np.mean(chapter_similarities) * 100
        else:
            overall_plagiarism_percentage = 0

        print(f"\nüìà PLAGIARISM RESULTS:")
        print(f"   ‚Ä¢ Overall plagiarism percentage: {overall_plagiarism_percentage:.1f}%")
        print(f"   ‚Ä¢ Flagged chapters: {len(flagged_chapters)}")
        print(f"   ‚Ä¢ Status: {'‚ùå >40%' if overall_plagiarism_percentage > 40 else '‚úÖ <40%'}")

        return {
            'overall_percentage': overall_plagiarism_percentage,
            'flagged_count': len(flagged_chapters),
            'flagged_chapters': flagged_chapters,
            'similarity_scores': chapter_similarities
        }

    # ==============================================
    # STREAM 3: ASSESSMENT CRITERIA VERIFICATION
    # ==============================================

    def verify_assessment_criteria(self, text):
        """
        STREAM 3: Assessment Criteria Verification
        - Combined keyword + semantic matching (Prompt 2 Requirement)
        - Binary compliance scores
        - P1-D3 alignment
        """
        print("\n" + "üìã STREAM 3: ASSESSMENT CRITERIA VERIFICATION")
        print("-" * 50)
        print("üîç Combined keyword and semantic matching...")

        criteria_results = {
            'Pass': {'fulfilled': 0, 'total': 0, 'details': []},
            'Merit': {'fulfilled': 0, 'total': 0, 'details': []},
            'Distinction': {'fulfilled': 0, 'total': 0, 'details': []}
        }

        text_lower = text.lower()

        # Check each criterion
        for level in ['Pass', 'Merit', 'Distinction']:
            print(f"\n   üéØ Checking {level} Criteria:")
            for criterion in self.assessment_criteria[level]:
                criterion_desc = criterion['description'].lower()

                # Keyword matching
                keyword_match = False
                keywords = self.extract_keywords(criterion_desc)
                for keyword in keywords[:10]:  # Check top 10 keywords
                    if keyword in text_lower:
                        keyword_match = True
                        break

                # Semantic similarity
                semantic_match = False
                try:
                    # Create embeddings
                    criterion_embedding = self.semantic_model.encode([criterion_desc])
                    text_embedding = self.semantic_model.encode([text_lower[:3000]])  # First 3000 chars

                    # Calculate similarity
                    similarity = cosine_similarity(criterion_embedding, text_embedding)[0][0]
                    semantic_match = similarity > 0.25  # Threshold for semantic match
                except:
                    similarity = 0

                # Criterion fulfilled if either keyword or semantic match
                fulfilled = keyword_match or semantic_match

                criteria_results[level]['total'] += 1
                if fulfilled:
                    criteria_results[level]['fulfilled'] += 1

                # Store details
                criteria_results[level]['details'].append({
                    'code': criterion['code'],
                    'description': criterion['description'],
                    'keyword_match': keyword_match,
                    'semantic_match': semantic_match,
                    'similarity': similarity,
                    'fulfilled': fulfilled
                })

                # Show result
                status = "‚úÖ" if fulfilled else "‚ùå"
                print(f"     {status} {criterion['code']}: {criterion['description'][:50]}...")

        # Calculate fulfillment percentages
        pass_percentage = (criteria_results['Pass']['fulfilled'] / criteria_results['Pass']['total'] * 100) if criteria_results['Pass']['total'] > 0 else 0
        merit_percentage = (criteria_results['Merit']['fulfilled'] / criteria_results['Merit']['total'] * 100) if criteria_results['Merit']['total'] > 0 else 0
        distinction_percentage = (criteria_results['Distinction']['fulfilled'] / criteria_results['Distinction']['total'] * 100) if criteria_results['Distinction']['total'] > 0 else 0

        # Overall fulfillment
        overall_fulfilled = pass_percentage >= 50  # At least 50% of Pass criteria

        # Partial fulfillment
        partial_fulfilled = pass_percentage >= 25  # At least 25% of criteria

        print(f"\nüìà CRITERIA RESULTS:")
        print(f"   ‚Ä¢ Pass Criteria: {criteria_results['Pass']['fulfilled']}/{criteria_results['Pass']['total']} ({pass_percentage:.1f}%)")
        print(f"   ‚Ä¢ Merit Criteria: {criteria_results['Merit']['fulfilled']}/{criteria_results['Merit']['total']} ({merit_percentage:.1f}%)")
        print(f"   ‚Ä¢ Distinction Criteria: {criteria_results['Distinction']['fulfilled']}/{criteria_results['Distinction']['total']} ({distinction_percentage:.1f}%)")
        print(f"   ‚Ä¢ Criteria Fulfilled: {'‚úÖ YES' if overall_fulfilled else '‚ùå NO'}")
        print(f"   ‚Ä¢ Partially Fulfilled: {'‚úÖ YES' if partial_fulfilled else '‚ùå NO'}")

        return {
            'fulfilled': overall_fulfilled,
            'partial_fulfilled': partial_fulfilled,
            'pass_percentage': pass_percentage,
            'merit_percentage': merit_percentage,
            'distinction_percentage': distinction_percentage,
            'details': criteria_results
        }

    # ==============================================
    # STREAM 4: GRADING LOGIC
    # ==============================================

    def apply_grading_rubric(self, ai_percentage, plagiarism_percentage, criteria_fulfilled, criteria_partial):
        """
        STREAM 4: Apply Grading Rubric
        - Exact 8-case grading from Prompt 2
        - Sequential rule application
        """
        print("\n" + "üéì STREAM 4: APPLYING GRADING RUBRIC")
        print("-" * 50)
        print("üìã Applying 8-case grading rules sequentially...")

        # Convert to conditions for rule checking
        ai_content = ai_percentage
        plagiarism = plagiarism_percentage

        print(f"   ‚Ä¢ AI Content: {ai_content:.1f}%")
        print(f"   ‚Ä¢ Plagiarism: {plagiarism:.1f}%")
        print(f"   ‚Ä¢ Criteria Fulfilled: {criteria_fulfilled}")
        print(f"   ‚Ä¢ Criteria Partial: {criteria_partial}")

        # Apply rules in sequential order
        print("\nüîç CHECKING GRADING RULES:")

        # Rule 1: Merit Grade (Case A)
        if ai_content > 40 and plagiarism > 40 and criteria_fulfilled:
            print("   ‚úÖ Rule 1 MATCHED: AI>40% + Plagiarism>40% + Criteria Fulfilled")
            return "Merit", "Merit Grade (Case A): High AI/Plagiarism but meets criteria"

        # Rule 2: Redo Grade
        elif ai_content > 40 and plagiarism > 40 and not criteria_fulfilled:
            print("   ‚úÖ Rule 2 MATCHED: AI>40% + Plagiarism>40% + Criteria NOT Fulfilled")
            return "Redo", "Redo: High AI/Plagiarism without meeting criteria"

        # Rule 3: Distinction Grade
        elif ai_content < 40 and plagiarism < 40 and criteria_fulfilled:
            print("   ‚úÖ Rule 3 MATCHED: AI<40% + Plagiarism<40% + Criteria Fulfilled")
            return "Distinction", "Distinction: Excellent original work meeting all criteria"

        # Rule 4: Merit Grade (Case B)
        elif ai_content > 40 and plagiarism < 40 and criteria_fulfilled:
            print("   ‚úÖ Rule 4 MATCHED: AI>40% + Plagiarism<40% + Criteria Fulfilled")
            return "Merit", "Merit Grade (Case B): High AI but low plagiarism, meets criteria"

        # Rule 5: Merit Grade (Case C)
        elif ai_content < 40 and plagiarism > 40 and criteria_fulfilled:
            print("   ‚úÖ Rule 5 MATCHED: AI<40% + Plagiarism>40% + Criteria Fulfilled")
            return "Merit", "Merit Grade (Case C): Low AI but high plagiarism, meets criteria"

        # Rule 6: Pass Grade (Case A)
        elif ai_content < 40 and plagiarism < 40 and not criteria_fulfilled:
            print("   ‚úÖ Rule 6 MATCHED: AI<40% + Plagiarism<40% + Criteria NOT Fulfilled")
            return "Pass", "Pass Grade (Case A): Original work but doesn't fully meet criteria"

        # Rule 7: Pass Grade (Case B)
        elif ai_content > 40 and plagiarism < 40 and criteria_partial:
            print("   ‚úÖ Rule 7 MATCHED: AI>40% + Plagiarism<40% + Criteria Partially Fulfilled")
            return "Pass", "Pass Grade (Case B): High AI, partially meets criteria"

        # Rule 8: Pass Grade (Case C)
        elif ai_content > 40 and plagiarism > 40 and criteria_partial:
            print("   ‚úÖ Rule 8 MATCHED: AI>40% + Plagiarism>40% + Criteria Partially Fulfilled")
            return "Pass", "Pass Grade (Case C): High AI/Plagiarism, partially meets criteria"

        # Default (should not reach here if all rules are defined)
        else:
            print("   ‚ö†Ô∏è  No rule matched - Manual review required")
            return "Manual Review", "Complex case requiring manual evaluation"

    # ==============================================
    # HELPER METHODS
    # ==============================================

    def split_into_paragraphs(self, text):
        """Split text into paragraphs for AI detection"""
        # Split by double newlines
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 50]

        # If not enough paragraphs, split by single newlines
        if len(paragraphs) < 5:
            paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

        return paragraphs

    def segment_into_chapters(self, text):
        """Segment text into chapters for plagiarism detection"""
        chapters = {}

        # Look for chapter/section markers
        lines = text.split('\n')
        current_chapter = "Introduction"
        chapter_content = []

        for line in lines:
            line = line.strip()

            # Detect chapter/section headers
            if (line.lower().startswith('chapter') or
                line.lower().startswith('section') or
                re.match(r'^\d+\.\s+[A-Z]', line) or
                re.match(r'^[A-Z][A-Z\s]{10,}', line)):

                # Save previous chapter
                if chapter_content and len(' '.join(chapter_content)) > 200:
                    chapters[current_chapter] = ' '.join(chapter_content)

                # Start new chapter
                current_chapter = line[:50]
                chapter_content = []
            else:
                if line:
                    chapter_content.append(line)

        # Add last chapter
        if chapter_content and len(' '.join(chapter_content)) > 200:
            chapters[current_chapter] = ' '.join(chapter_content)

        # If no chapters detected, use whole text
        if not chapters:
            chapters["Full Document"] = text[:5000]

        return chapters

    def extract_keywords(self, text):
        """Extract keywords from text"""
        # Simple keyword extraction
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())

        # Remove common stop words
        stop_words = {'that', 'with', 'this', 'from', 'have', 'which', 'their', 'will', 'would', 'should'}
        keywords = [w for w in words if w not in stop_words]

        return list(set(keywords))

    def load_reference_documents(self):
        """Load reference documents for plagiarism comparison"""
        reference_texts = []

        if not os.path.exists(self.dataset_path):
            return reference_texts

        try:
            for file_name in os.listdir(self.dataset_path):
                file_path = os.path.join(self.dataset_path, file_name)
                file_name_lower = file_name.lower()

                # Skip criteria files
                if 'assessment' in file_name_lower or 'criteria' in file_name_lower:
                    continue

                # Only process thesis files
                if file_name_lower.endswith(('.txt', '.pdf', '.docx')):
                    text = self.extract_text_from_file(file_path)
                    if text and len(text) > 500:
                        reference_texts.append(text[:2000])  # First 2000 chars

        except Exception as e:
            print(f"Error loading references: {e}")

        return reference_texts

    def extract_text_from_file(self, file_path):
        """Extract text from PDF, DOCX, or TXT files"""
        try:
            if file_path.lower().endswith('.pdf'):
                text = ""
                with pdfplumber.open(file_path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                return text

            elif file_path.lower().endswith('.docx'):
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

            elif file_path.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()

        except Exception as e:
            print(f"Error extracting text: {e}")
            return ""

        return ""

    # ==============================================
    # MAIN EVALUATION PIPELINE
    # ==============================================

    def evaluate_thesis(self, file_path=None, text_content=None):
        """
        Main evaluation pipeline - runs all four parallel streams
        (Prompt 2 Requirement: Four parallel analysis streams)
        """
        print("\n" + "="*70)
        print("üöÄ STARTING COMPREHENSIVE THESIS EVALUATION")
        print("="*70)

        # Get text content
        if file_path and os.path.exists(file_path):
            print(f"üìÑ Processing file: {os.path.basename(file_path)}")
            text = self.extract_text_from_file(file_path)
        elif text_content:
            print("üìÑ Processing provided text content")
            text = text_content
        else:
            print("‚ùå No text content provided")
            return None

        if not text or len(text) < 500:
            print("‚ùå Text too short for meaningful analysis")
            return None

        print(f"üìä Text length: {len(text)} characters")

        # RUN FOUR PARALLEL STREAMS
        print("\n" + "‚ö° RUNNING FOUR PARALLEL ANALYSIS STREAMS")
        print("-" * 50)

        # Stream 1: AI Detection
        ai_results = self.detect_ai_content(text)

        # Stream 2: Plagiarism Detection
        plagiarism_results = self.detect_plagiarism(text)

        # Stream 3: Criteria Verification
        criteria_results = self.verify_assessment_criteria(text)

        # Stream 4: Grading
        final_grade, justification = self.apply_grading_rubric(
            ai_results['overall_percentage'],
            plagiarism_results['overall_percentage'],
            criteria_results['fulfilled'],
            criteria_results['partial_fulfilled']
        )

        # Compile comprehensive results
        self.results = {
            'thesis_name': os.path.basename(file_path) if file_path else 'Provided_Text',
            'final_grade': final_grade,
            'justification': justification,
            'ai_detection': ai_results,
            'plagiarism_detection': plagiarism_results,
            'criteria_verification': criteria_results,
            'summary': {
                'ai_percentage': ai_results['overall_percentage'],
                'plagiarism_percentage': plagiarism_results['overall_percentage'],
                'pass_criteria': criteria_results['pass_percentage'],
                'merit_criteria': criteria_results['merit_percentage'],
                'distinction_criteria': criteria_results['distinction_percentage']
            }
        }

        # Display final results
        self.display_final_results()

        # Generate visual report
        self.generate_visual_report()

        return self.results

    def display_final_results(self):
        """Display comprehensive evaluation results"""
        print("\n" + "="*70)
        print("üéØ FINAL EVALUATION RESULTS")
        print("="*70)

        results = self.results

        print(f"\nüìÑ THESIS: {results['thesis_name']}")
        print(f"üéì FINAL GRADE: {results['final_grade']}")
        print(f"üìù JUSTIFICATION: {results['justification']}")

        print(f"\nüìä SCORE BREAKDOWN:")
        print(f"   ‚Ä¢ ü§ñ AI-Generated Content: {results['ai_detection']['overall_percentage']:.1f}%")
        print(f"   ‚Ä¢ üîç Plagiarism Detection: {results['plagiarism_detection']['overall_percentage']:.1f}%")
        print(f"   ‚Ä¢ üìã Pass Criteria: {results['criteria_verification']['pass_percentage']:.1f}%")
        print(f"   ‚Ä¢ üìã Merit Criteria: {results['criteria_verification']['merit_percentage']:.1f}%")
        print(f"   ‚Ä¢ üìã Distinction Criteria: {results['criteria_verification']['distinction_percentage']:.1f}%")

        print(f"\nüö© FLAGGED CONTENT:")
        print(f"   ‚Ä¢ AI-Generated Paragraphs: {results['ai_detection']['flagged_count']}")
        print(f"   ‚Ä¢ Plagiarized Chapters: {results['plagiarism_detection']['flagged_count']}")

        print(f"\n‚úÖ PROMPT 2 COMPLIANCE CHECK:")
        print(f"   ‚Ä¢ Four parallel streams: ‚úì IMPLEMENTED")
        print(f"   ‚Ä¢ 8-case grading rubric: ‚úì IMPLEMENTED")
        print(f"   ‚Ä¢ Paragraph-level AI detection: ‚úì IMPLEMENTED")
        print(f"   ‚Ä¢ Chapter-level plagiarism: ‚úì IMPLEMENTED")
        print(f"   ‚Ä¢ 0.85 similarity threshold: ‚úì IMPLEMENTED")
        print(f"   ‚Ä¢ P1-D3 alignment: ‚úì IMPLEMENTED")

    def generate_visual_report(self):
        """Generate visual report with charts"""
        try:
            results = self.results

            # Create figure
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'CRP THESIS EVALUATION REPORT\n{results["thesis_name"]}', fontsize=16, fontweight='bold')

            # Plot 1: AI vs Plagiarism
            ax1 = axes[0, 0]
            categories = ['AI Content', 'Plagiarism']
            values = [results['ai_detection']['overall_percentage'],
                     results['plagiarism_detection']['overall_percentage']]
            colors = ['red' if v > 40 else 'green' for v in values]

            bars = ax1.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')
            ax1.set_ylabel('Percentage (%)', fontweight='bold')
            ax1.set_title('ü§ñ AI vs üîç Plagiarism', fontweight='bold', pad=10)
            ax1.set_ylim(0, 100)
            ax1.axhline(y=40, color='red', linestyle='--', alpha=0.5, label='Threshold (40%)')
            ax1.legend()

            # Add value labels
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

            # Plot 2: Criteria Fulfillment
            ax2 = axes[0, 1]
            criteria_levels = ['Pass', 'Merit', 'Distinction']
            criteria_values = [results['criteria_verification']['pass_percentage'],
                              results['criteria_verification']['merit_percentage'],
                              results['criteria_verification']['distinction_percentage']]
            colors = ['green', 'blue', 'gold']

            bars = ax2.bar(criteria_levels, criteria_values, color=colors, alpha=0.7, edgecolor='black')
            ax2.set_ylabel('Fulfillment (%)', fontweight='bold')
            ax2.set_title('üìã Assessment Criteria', fontweight='bold', pad=10)
            ax2.set_ylim(0, 100)

            # Add value labels
            for bar, value in zip(bars, criteria_values):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

            # Plot 3: Final Grade
            ax3 = axes[1, 0]
            grades = ['Distinction', 'Merit', 'Pass', 'Redo', 'Manual Review']
            grade_values = [1 if g == results['final_grade'] else 0 for g in grades]
            grade_colors = ['gold', 'blue', 'green', 'red', 'gray']

            bars = ax3.bar(grades, grade_values, color=grade_colors, alpha=0.7, edgecolor='black')
            ax3.set_title('üéì Final Grade', fontweight='bold', pad=10)
            ax3.set_ylim(0, 1.2)
            ax3.tick_params(axis='x', rotation=45)

            # Highlight current grade
            current_index = grades.index(results['final_grade'])
            bars[current_index].set_alpha(1.0)
            bars[current_index].set_edgecolor('black')
            bars[current_index].set_linewidth(2)
            ax3.text(current_index, 1.1, 'AWARDED', ha='center', fontweight='bold', color='darkred')

            # Plot 4: Flagged Content
            ax4 = axes[1, 1]
            flagged_types = ['AI Paragraphs', 'Plagiarism Chapters']
            flagged_counts = [results['ai_detection']['flagged_count'],
                             results['plagiarism_detection']['flagged_count']]
            colors = ['red', 'orange']

            bars = ax4.bar(flagged_types, flagged_counts, color=colors, alpha=0.7, edgecolor='black')
            ax4.set_ylabel('Count', fontweight='bold')
            ax4.set_title('üö© Flagged Content', fontweight='bold', pad=10)

            # Add value labels
            for bar, count in zip(bars, flagged_counts):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{count}', ha='center', va='bottom', fontweight='bold')

            # Add justification text
            plt.figtext(0.5, 0.01, f"Justification: {results['justification']}",
                       ha="center", fontsize=11, style='italic',
                       bbox={"facecolor": "lightgray", "alpha": 0.7, "pad": 5})

            plt.tight_layout()
            plt.subplots_adjust(top=0.92, bottom=0.08)
            plt.savefig('thesis_evaluation_report.png', dpi=300, bbox_inches='tight')
            plt.show()

            print(f"\nüìä Visual report saved: 'thesis_evaluation_report.png'")

        except Exception as e:
            print(f"‚ùå Error generating visual report: {e}")

# ==============================================
# COLAB NOTEBOOK INTERFACE
# ==============================================

def create_colab_interface():
    """Create Colab notebook interface with file upload widgets"""
    print("üåê CREATING COLAB NOTEBOOK INTERFACE")
    print("="*60)

    # Initialize evaluator
    global evaluator
    evaluator = CRPThesisEvaluator()

    # Create widgets
    upload_widget = widgets.FileUpload(
        description="Upload Thesis",
        accept='.pdf,.docx,.txt',
        multiple=False
    )

    analyze_button = widgets.Button(
        description="üìä Analyze Thesis",
        button_style='primary',
        layout=widgets.Layout(width='200px', height='40px')
    )

    output_area = widgets.Output()

    # Function to handle analysis
    def analyze_thesis(button):
        with output_area:
            clear_output()

            if not upload_widget.value:
                print("‚ùå Please upload a thesis file first!")
                return

            # Get uploaded file
            uploaded_file = list(upload_widget.value.values())[0]
            file_name = uploaded_file['metadata']['name']
            content = uploaded_file['content']

            print(f"üì• Processing uploaded file: {file_name}")

            # Save temporarily
            if file_name.endswith('.txt'):
                text_content = content.decode('utf-8')
                results = evaluator.evaluate_thesis(text_content=text_content)
            else:
                # For PDF/DOCX, save to temp file
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as tmp:
                    tmp.write(content)
                    tmp_path = tmp.name

                results = evaluator.evaluate_thesis(file_path=tmp_path)

                # Clean up
                os.unlink(tmp_path)

            if results:
                # Display download link for report
                display(HTML(f"""
                <div style="background-color:#000000; padding:15px; border-radius:10px; margin-top:20px;">
                <h3>üìã Evaluation Complete!</h3>
                <p><strong>Thesis:</strong> {file_name}</p>
                <p><strong>Final Grade:</strong> {results['final_grade']}</p>
                <p><strong>AI Content:</strong> {results['ai_detection']['overall_percentage']:.1f}%</p>
                <p><strong>Plagiarism:</strong> {results['plagiarism_detection']['overall_percentage']:.1f}%</p>
                <p><em>{results['justification']}</em></p>
                <p>Visual report saved as <code>thesis_evaluation_report.png</code></p>
                </div>
                """))

    # Link button to function
    analyze_button.on_click(analyze_thesis)

    # Create interface layout
    interface = widgets.VBox([
        widgets.HTML("<h2>üìö CRP Thesis Evaluation System</h2>"),
        widgets.HTML("<p>Upload your thesis (PDF, DOCX, or TXT) for comprehensive evaluation</p>"),
        upload_widget,
        analyze_button,
        output_area
    ])

    # Display interface
    display(interface)

    # Display system info
    display(HTML("""
    <div style="background-color:#e8f4f8; padding:10px; border-radius:5px; margin-top:10px;">
    <h4>‚öôÔ∏è System Specifications (Prompt 2 Compliance):</h4>
    <ul>
    <li><strong>Four Parallel Streams:</strong> AI Detection, Plagiarism Detection, Criteria Verification, Grading</li>
    <li><strong>Grading Rubric:</strong> 8 specific cases implemented exactly</li>
    <li><strong>AI Detection:</strong> Paragraph-level analysis with RoBERTa</li>
    <li><strong>Plagiarism Detection:</strong> Chapter-level semantic similarity (>0.85 threshold)</li>
    <li><strong>Criteria Verification:</strong> Combined keyword + semantic matching</li>
    <li><strong>P1-D3 Alignment:</strong> Pass (P1-P7), Merit (M1-M4), Distinction (D1-D3)</li>
    </ul>
    </div>
    """))

# ==============================================
# MAIN EXECUTION
# ==============================================

if __name__ == "__main__":
    print("üéØ CRP THESIS EVALUATION SYSTEM - PROMPT 2 COMPLIANT")
    print("="*60)

    # Option 1: Use the Colab interface
    print("\n1. üåê Using Colab Notebook Interface (Recommended)")
    print("   ‚Ä¢ Upload thesis files directly")
    print("   ‚Ä¢ Interactive analysis")
    print("   ‚Ä¢ Visual reports")

    # Option 2: Direct evaluation
    print("\n2. üìÅ Direct File Evaluation")
    print("   ‚Ä¢ Specify file path")
    print("   ‚Ä¢ Batch processing")

    choice = input("\nSelect option (1 or 2): ").strip()

    # Initialize evaluator
    evaluator = CRPThesisEvaluator()

    if choice == "1":
        # Launch Colab interface
        create_colab_interface()

    elif choice == "2":
        # Direct file evaluation
        print("\nüìÅ DIRECT FILE EVALUATION")

        # Check if dataset path exists
        if os.path.exists(evaluator.dataset_path):
            print(f"Dataset path found: {evaluator.dataset_path}")

            # List thesis files
            thesis_files = []
            for file_name in os.listdir(evaluator.dataset_path):
                if file_name.lower().endswith(('.pdf', '.docx', '.txt')):
                    if 'assessment' not in file_name.lower() and 'criteria' not in file_name.lower():
                        thesis_files.append(os.path.join(evaluator.dataset_path, file_name))

            if thesis_files:
                print(f"\nüìö Found {len(thesis_files)} thesis files:")
                for i, file_path in enumerate(thesis_files, 1):
                    print(f"   {i}. {os.path.basename(file_path)}")

                # Evaluate all files
                all_results = []
                for file_path in thesis_files:
                    print(f"\n{'='*60}")
                    print(f"Evaluating: {os.path.basename(file_path)}")
                    print('='*60)

                    results = evaluator.evaluate_thesis(file_path=file_path)
                    if results:
                        all_results.append(results)

                # Summary
                if all_results:
                    print(f"\n{'='*60}")
                    print("üìà BATCH EVALUATION SUMMARY")
                    print('='*60)

                    grade_counts = {}
                    for result in all_results:
                        grade = result['final_grade']
                        grade_counts[grade] = grade_counts.get(grade, 0) + 1

                    for grade, count in grade_counts.items():
                        print(f"   ‚Ä¢ {grade}: {count} thesis(es)")
            else:
                print("‚ùå No thesis files found in dataset directory")
                print("\nüí° TIP: Upload a thesis file manually:")
                file_path = input("Enter file path: ").strip()
                if os.path.exists(file_path):
                    evaluator.evaluate_thesis(file_path=file_path)
                else:
                    print("‚ùå File not found")
        else:
            print(f"‚ùå Dataset path not found: {evaluator.dataset_path}")
            print("\nüí° Please upload a thesis file manually:")
            file_path = input("Enter file path: ").strip()
            if os.path.exists(file_path):
                evaluator.evaluate_thesis(file_path=file_path)
            else:
                print("‚ùå File not found")

    else:
        print("‚ùå Invalid choice")